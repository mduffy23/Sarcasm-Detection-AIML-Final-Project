# Sarcasm Detection

This project leverages a dataset from https://www.kaggle.com/datasets/danofer/sarcasm to train sarcasm detection models. The dataset contains sarcastic and not sarcastic reddit posts, which give a solid representation of how people use sarcasm in textual conversation. For modelling, Term Frequency-Inverse Document Frequency (TF-IDF) was trained as a baseline model and a fine tuned RoBERTa model was trained as the final model. 

- TF-IDF evaluates how important a word is to a document in a collection, or corpus by multiplying the term frequency (TF)—how often a word appears in a document—by the inverse document frequency (IDF), which measures how rare a word is across all documents. Certain *give away* phrases in sarcastic statements should be caught by this method, which make it a viable solution, but TF-IDF will not be able to pick up on word order, context, and sentiment contrast, which are all important for sarcasm detection. Sarcasm can often be subtle, and a model needs to understand these concepts to be viable. One the TF-IDF vectors are applied, a logistic regression is used for modelling.

- RoBERTa is a large contextual language transformer model which solves a lot of the shortcomings of TF-IDF. RoBERTa has the same architecture as BERT, but was trained on ten times more data, which makes it generally better than BERT. RoBERTa has a few other training improvements like dynamic masking (versus static masking), larger batches, and more training iterations, which also lead to it performing better than the original BERT model. Because of all these training methods, RoBERTa is a great model to fine tune for sarcasm detection, as it can pick up on subtle context changes based on sentence structure, punctuation, and contradictions because it is trained on such a large corpus of text. 

# Methods

- For TF-IDF, the comments were lemmatized (transforms words that mean the same thing but are in different parts of speech to be the same for the tokenizer) to help with understanding similar words. RoBERTa on the other hand does not need the words lemmatized since it uses a subword tokenizer and contextual embeddings, allowing RoBERTa to understand relationships between word forms based on shared subwords, without explicit lemmatization. Similar words like run, runs, ran, and running will have similar vectors when they appear in similar contexts. Since RoBERTa was trained on raw unaltered text, it should actually perform better on the raw statements; it understands things like tense and tone. 

- Both models were given a secondary variable that measures if the statement has common patterns of sarcasm. This is not a hack to say all statements that start with "Oh yeah," are sarcastic, but rather a baseline understanding that these particular, words, phrases, and punctuation are often used when typing out sarcasm, trying to further guide the models into understanding what makes a statement sarcastic. Conventions such as elongated spelling ("woooow"), exaggeration words ("literally"), and ironic punctuation ("!!") are set as *tells* for sarcasm in statements.

- RoBERTa is a huggingface transformer model, but to utilize the lexical features, a torch model was defined to leverage the pretrained RoBERTa model and then add the lexical features to the model. The model was then hyperparameter tuned with the optuna library, which offered the best learning rate, dropout, and weight decay. The regular transformer training arguments and trainer were used to fine tune the model to predict sarcasm!

- This model was saved for usage in a small we application. FastAPI was used to generate a simple endpoint to hit the model from an HTML page. Hoping to deploy this on render soon, but the model may be too large for the basic version.